{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "im21SHT_RfzT"
      },
      "source": [
        "## Recommender Systems\n",
        "\n",
        "**Recommender Systems** are algorithms aimed at suggesting relevant items to users (items being movies to watch, text to read, products to buy or anything else depending on the product).\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1920/1*Y_QG3Kvfk0fSnCirLBHZ7w.jpeg)\n",
        "\n",
        "\n",
        "### Recommendation paradigms\n",
        "\n",
        "The distinction between approaches is more academic than practical, but it’s important to understand their differences.\n",
        "Broadly speaking, recommender systems are of 4 types:\n",
        "\n",
        "1. **Collaborative filtering** is perhaps the most well-known approach to recommendation, to the point that it’s sometimes seen as synonymous with the field. The main idea is that you’re given a matrix of preferences by users for items, and these are used to predict missing preferences and recommend items with high predictions. All you need to get started is user and item IDs and a notion of preference by users for items (ratings, views, etc.).\n",
        "\n",
        "2. **Content-based filtering** algorithms are given user preferences for items and recommend similar items based on a domain-specific notion of item content. This approach also extends naturally to cases where item metadata is available (e.g., movie stars, book authors, and music genres).\n",
        "3. **Social and demographic** recommenders suggest items that are liked by friends, friends of friends, and demographically-similar people. Such recommenders don’t need any preferences by the user to whom recommendations are made, making them very powerful.\n",
        "4. **Contextual recommendation** algorithms recommend items that match the user’s current context. This allows them to be more flexible and adaptive to current user needs than methods that ignore context (essentially giving the same weight to all of the user’s history). Hence, contextual algorithms are more likely to elicit a response than approaches that are based only on historical data.\n",
        "\n",
        "## Collaborative Filtering\n",
        "\n",
        "Collaborative filtering (CF) systems work by collecting user feedback in the form of ratings for items in a given domain and exploiting similarities in rating behavior among several users in determining how to recommend an item.\n",
        "CF accumulates customer product ratings, identifies customers with common ratings, and offers recommendations based on inter-customer comparisons. It’s based on the idea that people who agree in their evaluations of certain items in the past are likely to agree again in the future. For example, most people ask their trusted friends for restaurant or movie suggestions.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/687/1*-Jr1l2rlj9SBcCzlDHtN5g.jpeg)\n",
        "\n",
        "Collaborative filtering models are based on an assumption that people like things similar to other things they like, and things that are liked by other people with similar taste.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1348/1*K5BOY3B93MLn173VVzOW0Q.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix Factorization\n",
        "![alt text](https://datascienceplus.com/wp-content/uploads/2017/09/2017-09-20-2.png)"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "VO3DQ4scRfzn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3eC2d6DBRfz0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "source": [
        "ratings = pd.read_csv('ratings_mat.csv')\n",
        "del ratings['Unnamed: 0']\n",
        "ratings.fillna(-1)\n",
        "\n",
        "ratings = ratings.values\n",
        "exist = (ratings != -1).astype(int)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qaTVHvKmRfz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "source": [
        "n_factors = 50\n",
        "n_users, n_movies = ratings.shape\n",
        "epochs = 250\n",
        "logging_rate = 10"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9h0NdKaIRfz4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "source": [
        "ratings = torch.tensor(ratings)\n",
        "exist = torch.tensor(exist)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RIPd4cmjRfz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ratings[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBT37hKrKC23",
        "outputId": "a16d03ba-4d9c-4775-db7f-5f7d264c7b72"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5,  3,  4,  ..., -1, -1, -1],\n",
            "        [ 4, -1, -1,  ..., -1, -1, -1],\n",
            "        [-1, -1, -1,  ..., -1, -1, -1],\n",
            "        [-1, -1, -1,  ..., -1, -1, -1],\n",
            "        [ 4,  3, -1,  ..., -1, -1, -1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task:\n",
        "\n",
        "1. Define the two trainable latent factor tensors. One for the user and one for the movies.\n",
        "2. Define the optimizer for the training.\n",
        "3. Define the function that calculate the loss.\n",
        "4. Calculate the estimated rating matrix.\n",
        "5. Adjust the range of the estimated rating matrix to be [1, 5]."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "W6dZr7CXRfz5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "source": [
        "user_factors = torch.rand(n_factors, n_users, requires_grad=True)\n",
        "movies_factors = torch.rand(n_factors, n_movies, requires_grad=True)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7qiS1TGvRfz5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at epoch 0 = 5.326584339141846\n",
            "Loss at epoch 10 = 5.2930169105529785\n",
            "Loss at epoch 20 = 5.199587345123291\n",
            "Loss at epoch 30 = 5.049565315246582\n",
            "Loss at epoch 40 = 4.848113059997559\n",
            "Loss at epoch 50 = 4.601992130279541\n",
            "Loss at epoch 60 = 4.31919002532959\n",
            "Loss at epoch 70 = 4.008499622344971\n",
            "Loss at epoch 80 = 3.6790778636932373\n",
            "Loss at epoch 90 = 3.3400299549102783\n",
            "Loss at epoch 100 = 3.0000295639038086\n",
            "Loss at epoch 110 = 2.6670033931732178\n",
            "Loss at epoch 120 = 2.347891092300415\n",
            "Loss at epoch 130 = 2.0484859943389893\n",
            "Loss at epoch 140 = 1.7733550071716309\n",
            "Loss at epoch 150 = 1.5258251428604126\n",
            "Loss at epoch 160 = 1.308035969734192\n",
            "Loss at epoch 170 = 1.1210335493087769\n",
            "Loss at epoch 180 = 0.9648990631103516\n",
            "Loss at epoch 190 = 0.8388968706130981\n",
            "Loss at epoch 200 = 0.7416288256645203\n",
            "Loss at epoch 210 = 0.6711893677711487\n",
            "Loss at epoch 220 = 0.625308632850647\n",
            "Loss at epoch 230 = 0.6014841198921204\n",
            "Loss at epoch 240 = 0.5970937013626099\n",
            "Last Loss = 0.6075699925422668\n"
          ]
        }
      ],
      "source": [
        "optimizer = optim.SGD(params=(user_factors, movies_factors), lr=0.01)\n",
        "\n",
        "def calc_loss(true_r, est_r, exist):\n",
        "    true_exist = true_r*exist\n",
        "    est_exist = est_r*exist\n",
        "    diff = true_exist - est_exist\n",
        "    res = torch.mean(torch.square(diff))\n",
        "    return res\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    user_factors_tran = torch.transpose(user_factors, 0, 1)\n",
        "    estimated = torch.matmul(user_factors_tran, movies_factors)\n",
        "    loss = calc_loss(ratings, estimated, exist)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % logging_rate == 0:\n",
        "        print(\"Loss at epoch {} = {}\".format(epoch, loss.item()))\n",
        "print(\"Last Loss = {}\".format(loss.item()))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92AwhbZ7Rfz6",
        "outputId": "39dea69f-7619-46b1-e4a6-3bfe3998645c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save the ratings:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FcimNMsqRfz6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "source": [
        "estimated = torch.transpose(user_factors, 0, 1).mm(movies_factors)\n",
        "estimated = estimated.cpu().detach().numpy()\n",
        "max_r, min_r = estimated.max(), estimated.min()\n",
        "estimated = 1 + 4 * (estimated - min_r) / (max_r - min_r)\n",
        "n_users, n_movies = estimated.shape\n",
        "estimated = pd.DataFrame(estimated, columns=list(range(1, n_movies + 1)),\n",
        "                       index=list(range(1, n_users + 1)))\n",
        "estimated.to_csv(\"answer_ratings.csv\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Pw_8yzfVRfz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to make it nonlinear and Deeper?\n",
        "\n",
        "Answer: Using deep NN, but will be the input in this case?\n",
        "The user and item."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "beMgucLLRfz8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "batch_sz = 128"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nswulT2MRfz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "ratings = pd.read_csv('ratings.csv')\n",
        "users = ratings['UserID'].values - 1\n",
        "movies = ratings['MovieID'].values - 1\n",
        "rates = ratings['Rating'].values\n",
        "n_samples = len(rates)\n",
        "batches = []\n",
        "for i in range(0, n_samples, batch_sz):\n",
        "    limit =  min(i + batch_sz, n_samples)\n",
        "    users_batch, movies_batch, rates_batch = users[i: limit], \\\n",
        "                                             movies[i: limit],\\\n",
        "                                             rates[i: limit]\n",
        "    batches.append((torch.tensor(users_batch, dtype=torch.long).to(device),\n",
        "                    torch.tensor(movies_batch, dtype=torch.long).to(device),\n",
        "                    torch.tensor(rates_batch, dtype=torch.float).to(device)))\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ymAvQCk7Rfz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition\n",
        "Tasks:\n",
        "1. Define the needed modules for the model:\n",
        "        1.1 Two `Embedding`s for users and movies.\n",
        "        1.2 One `Dropout` for the output of the embeddings.\n",
        "        1.3 The hidden layers using `get_layer` function and `hidden` argument.\n",
        "\n",
        "\n",
        "\n",
        "2. Define the flow of the training of the model in `forward()` function.\n",
        "        2.1 Get the 2 embeddings tensors, then concatenate both.\n",
        "        2.2 Run it through the hidden layers then the last fc layer.\n",
        "        2.3 Apply sigmoid activation.\n",
        "        2.4 Adjust the range of the estimated rating matrix to be [1, 5]."
      ],
      "metadata": {
        "collapsed": false,
        "id": "tQf5Uf3bRfz_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "source": [
        "class RecommenderNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Creates a dense network with embedding layers.\n",
        "    Args:\n",
        "        n_users:\n",
        "            Number of unique users in the dataset.\n",
        "        n_movies:\n",
        "            Number of unique movies in the dataset.\n",
        "        n_factors:\n",
        "            Number of columns in the embeddings matrix.\n",
        "        embedding_dropout:\n",
        "            Dropout rate to apply right after embeddings layer.\n",
        "        hidden:\n",
        "            A single integer or a list of integers defining the number of\n",
        "            units in hidden layer(s).\n",
        "        dropout_rate:\n",
        "            dropout rate after each hidden layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_users, n_movies,\n",
        "                 n_factors=50, embedding_dropout=0.02,\n",
        "                 hidden=10, dropout_rate=0.2):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        n_last = hidden[-1]\n",
        "\n",
        "        def gen_layers(n_in):\n",
        "            \"\"\"\n",
        "            A generator that yields a sequence of hidden layers and\n",
        "            their activations/dropouts.\n",
        "\n",
        "            Note that the function captures `hidden` and `dropouts`\n",
        "            values from the outer scope.\n",
        "            \"\"\"\n",
        "            hidden_layers = []\n",
        "            for n_out in hidden:\n",
        "                hidden_layers.append(nn.Linear(n_in, n_out))\n",
        "                hidden_layers.append(nn.ReLU())\n",
        "                if dropout_rate is not None and dropout_rate > 0.:\n",
        "                     hidden_layers.append(nn.Dropout(dropout_rate))\n",
        "                n_in = n_out\n",
        "            return hidden_layers\n",
        "\n",
        "        self.users = nn.Embedding(n_users, n_factors)\n",
        "        self.movies = nn.Embedding(n_movies, n_factors)\n",
        "        self.drop = nn.Dropout(embedding_dropout)\n",
        "        self.hidden = gen_layers(n_factors * 2)\n",
        "        \n",
        "        self.fc = nn.Linear(n_last, 1)\n",
        "        self._init()\n",
        "\n",
        "\n",
        "    def forward(self, users, movies, minmax=None):\n",
        "        users = self.users(users)\n",
        "        movies = self.movies(movies)\n",
        "        x = torch.cat([users, movies], dim=1)\n",
        "        x = self.drop(x)\n",
        "        for hidden in self.hidden:\n",
        "            x = hidden(x)\n",
        "        x = torch.sigmoid(self.fc(x))\n",
        "        return x * 4 + 1\n",
        "\n",
        "\n",
        "    def _init(self):\n",
        "        \"\"\"\n",
        "        Setup embeddings and hidden layers with reasonable initial values.\n",
        "        \"\"\"\n",
        "\n",
        "        def init(m):\n",
        "            if type(m) == nn.Linear:\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                m.bias.data.fill_(0.01)\n",
        "\n",
        "        self.users.weight.data.uniform_(-0.05, 0.05)\n",
        "        self.movies.weight.data.uniform_(-0.05, 0.05)\n",
        "        for layer in self.hidden:\n",
        "            init(layer)\n",
        "        init(self.fc)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "CUmRl93FRf0A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at epoch 0 = 1.2321830987930298\n",
            "Loss at epoch 1 = 1.037410020828247\n",
            "Loss at epoch 2 = 0.9033569097518921\n",
            "Loss at epoch 3 = 0.9484357237815857\n",
            "Loss at epoch 4 = 0.8854527473449707\n",
            "Loss at epoch 5 = 0.817359209060669\n",
            "Loss at epoch 6 = 0.8303821086883545\n",
            "Loss at epoch 7 = 0.8551475405693054\n",
            "Loss at epoch 8 = 0.9294219613075256\n",
            "Loss at epoch 9 = 0.7396958470344543\n",
            "Loss at epoch 10 = 0.7679277062416077\n",
            "Loss at epoch 11 = 0.7623749375343323\n",
            "Loss at epoch 12 = 0.7428615093231201\n",
            "Loss at epoch 13 = 0.7214045524597168\n",
            "Loss at epoch 14 = 0.7749396562576294\n",
            "Loss at epoch 15 = 0.7848702073097229\n",
            "Loss at epoch 16 = 0.5242792367935181\n",
            "Loss at epoch 17 = 0.6891729235649109\n",
            "Loss at epoch 18 = 0.5654334425926208\n",
            "Loss at epoch 19 = 0.7867417335510254\n",
            "Loss at epoch 20 = 0.6534708142280579\n",
            "Loss at epoch 21 = 0.607595682144165\n",
            "Loss at epoch 22 = 0.737130343914032\n",
            "Loss at epoch 23 = 0.6620026230812073\n",
            "Loss at epoch 24 = 0.6156119108200073\n",
            "Loss at epoch 25 = 0.7351477742195129\n",
            "Loss at epoch 26 = 0.5994512438774109\n",
            "Loss at epoch 27 = 0.8317376375198364\n",
            "Loss at epoch 28 = 0.7528547048568726\n",
            "Loss at epoch 29 = 0.7750998735427856\n",
            "Loss at epoch 30 = 0.6686445474624634\n",
            "Loss at epoch 31 = 0.6752011775970459\n",
            "Loss at epoch 32 = 0.5802232027053833\n",
            "Loss at epoch 33 = 0.6270166039466858\n",
            "Loss at epoch 34 = 0.5904796719551086\n",
            "Loss at epoch 35 = 0.664161741733551\n",
            "Loss at epoch 36 = 0.5289806723594666\n",
            "Loss at epoch 37 = 0.6977507472038269\n",
            "Loss at epoch 38 = 0.5746822953224182\n",
            "Loss at epoch 39 = 0.626254141330719\n",
            "Loss at epoch 40 = 0.6032029986381531\n",
            "Loss at epoch 41 = 0.6542074680328369\n",
            "Loss at epoch 42 = 0.591759979724884\n",
            "Loss at epoch 43 = 0.682491660118103\n",
            "Loss at epoch 44 = 0.5991067886352539\n",
            "Loss at epoch 45 = 0.7314702868461609\n",
            "Loss at epoch 46 = 0.6739031672477722\n",
            "Loss at epoch 47 = 0.6206690073013306\n",
            "Loss at epoch 48 = 0.6248801350593567\n",
            "Loss at epoch 49 = 0.6667296290397644\n",
            "Loss at epoch 50 = 0.7923516631126404\n",
            "Loss at epoch 51 = 0.5712358355522156\n",
            "Loss at epoch 52 = 0.7763044238090515\n",
            "Loss at epoch 53 = 0.6911676526069641\n",
            "Loss at epoch 54 = 0.8212337493896484\n",
            "Loss at epoch 55 = 0.70204097032547\n",
            "Loss at epoch 56 = 0.6767980456352234\n",
            "Loss at epoch 57 = 0.6489685773849487\n",
            "Loss at epoch 58 = 0.6421962976455688\n",
            "Loss at epoch 59 = 0.6161389946937561\n",
            "Loss at epoch 60 = 0.7221682071685791\n",
            "Loss at epoch 61 = 0.6064122319221497\n",
            "Loss at epoch 62 = 0.7713874578475952\n",
            "Loss at epoch 63 = 0.6068907380104065\n",
            "Loss at epoch 64 = 0.7443504333496094\n",
            "Loss at epoch 65 = 0.6915072202682495\n",
            "Loss at epoch 66 = 0.6759721636772156\n",
            "Loss at epoch 67 = 0.6704110503196716\n",
            "Loss at epoch 68 = 0.6346876621246338\n",
            "Loss at epoch 69 = 0.7087225317955017\n",
            "Loss at epoch 70 = 0.6019139289855957\n",
            "Loss at epoch 71 = 0.7962188720703125\n",
            "Loss at epoch 72 = 0.5914677977561951\n",
            "Loss at epoch 73 = 0.5816750526428223\n",
            "Loss at epoch 74 = 0.7203638553619385\n",
            "Loss at epoch 75 = 0.8059867024421692\n",
            "Loss at epoch 76 = 0.7661187052726746\n",
            "Loss at epoch 77 = 0.6496539115905762\n",
            "Loss at epoch 78 = 0.6851246953010559\n",
            "Loss at epoch 79 = 0.7778908610343933\n",
            "Loss at epoch 80 = 0.7429667711257935\n",
            "Loss at epoch 81 = 0.6378099918365479\n",
            "Loss at epoch 82 = 0.795096218585968\n",
            "Loss at epoch 83 = 0.5911409854888916\n",
            "Loss at epoch 84 = 0.6124672889709473\n",
            "Loss at epoch 85 = 0.7111616134643555\n",
            "Loss at epoch 86 = 0.5603611469268799\n",
            "Loss at epoch 87 = 0.6521711349487305\n",
            "Loss at epoch 88 = 0.749458372592926\n",
            "Loss at epoch 89 = 0.624242901802063\n",
            "Loss at epoch 90 = 0.7690945863723755\n",
            "Loss at epoch 91 = 0.7021586894989014\n",
            "Loss at epoch 92 = 0.6930925250053406\n",
            "Loss at epoch 93 = 0.6715320348739624\n",
            "Loss at epoch 94 = 0.6499978303909302\n",
            "Loss at epoch 95 = 0.7073668241500854\n",
            "Loss at epoch 96 = 0.6569381356239319\n",
            "Loss at epoch 97 = 0.7391020059585571\n",
            "Loss at epoch 98 = 0.5877532958984375\n",
            "Loss at epoch 99 = 0.6464013457298279\n",
            "Last Loss = 0.6464013457298279\n"
          ]
        }
      ],
      "source": [
        "net = RecommenderNet(n_users, n_movies, hidden=[128, 256, 128])\n",
        "criterion = nn.MSELoss(reduction='mean')\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=2)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    for users_batch, movies_batch, rates_batch in batches:\n",
        "        net.zero_grad()\n",
        "        out = net(users_batch, movies_batch, [1, 5]).squeeze()\n",
        "        loss = criterion(rates_batch, out)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss\n",
        "    scheduler.step(loss)\n",
        "    print(\"Loss at epoch {} = {}\".format(epoch, loss.item()))\n",
        "print(\"Last Loss = {}\".format(loss.item()))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rmsakrEXRf0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98eef6bb-0fb8-4f14-f532-d320a56b4a80"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss = 0.7456024885177612\n",
            "Accuracy = 0.44933000206947327\n"
          ]
        }
      ],
      "source": [
        "# evaluate the model\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    total_correct = 0\n",
        "    n_correct = 0\n",
        "    total_loss = 0\n",
        "    n_loss = 0\n",
        "    for users_batch, movies_batch, rates_batch in batches:\n",
        "        out = net(users_batch, movies_batch, [1, 5]).squeeze()\n",
        "        loss = criterion(rates_batch, out)\n",
        "        total_loss += loss\n",
        "        n_loss += 1\n",
        "        total_correct += torch.sum(torch.abs(rates_batch - out) < 0.5)\n",
        "        n_correct += len(rates_batch)\n",
        "    print(\"Average loss = {}\".format(total_loss / n_loss))\n",
        "    print(\"Accuracy = {}\".format(total_correct / n_correct))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "82jn1CCoRf0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0dd885-3b54-4deb-d9f9-f88017a2d75f"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}