{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"oWBU5sLUrxgg"},"source":["**POS-Tagging**\n","\n","Welcome to the forth lab! In this excercise you will build a simple pos-tagger.\n","The excercise is inspired from Pytorch tutorial site: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOlNcPP3rxg1","outputId":"580556b1-b5aa-41ca-9e76-ccaa17c47a15","executionInfo":{"status":"ok","timestamp":1663921450741,"user_tz":-180,"elapsed":3011,"user":{"displayName":"Айдар Валеев","userId":"02469296822751141251"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","torch.manual_seed(1)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f64a715a0f0>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"5mELvN9xrxg8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663921452124,"user_tz":-180,"elapsed":516,"user":{"displayName":"Айдар Валеев","userId":"02469296822751141251"}},"outputId":"f9f182a3-587f-4783-9184-b872cb0b89c3"},"source":["lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n","inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n","\n","# initialize the hidden state.\n","hidden = (torch.randn(1, 1, 3),\n","          torch.randn(1, 1, 3))\n","for i in inputs:\n","    # Step through the sequence one element at a time.\n","    # after each step, hidden contains the hidden state.\n","    out, hidden = lstm(i.view(1, 1, -1), hidden)\n","\n","# alternatively, we can do the entire sequence all at once.\n","# the first value returned by LSTM is all of the hidden states throughout\n","# the sequence. the second is just the most recent hidden state\n","# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n","# The reason for this is that:\n","# \"out\" will give you access to all hidden states in the sequence\n","# \"hidden\" will allow you to continue the sequence and backpropagate,\n","# by passing it as an argument  to the lstm at a later time\n","# Add the extra 2nd dimension\n","inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n","hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n","out, hidden = lstm(inputs, hidden)\n","print(out)\n","print(hidden)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.0187,  0.1713, -0.2944]],\n","\n","        [[-0.3521,  0.1026, -0.2971]],\n","\n","        [[-0.3191,  0.0781, -0.1957]],\n","\n","        [[-0.1634,  0.0941, -0.1637]],\n","\n","        [[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward0>)\n","(tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward0>), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=<StackBackward0>))\n"]}]},{"cell_type":"markdown","metadata":{"id":"A5TJTreMrxg_"},"source":["**Task:**\n","\n","Load the `training_data` from `corpus-small.train`, and modify the `tag_to_ix` dictionary to have all different tags."]},{"cell_type":"code","source":["!head -n 5 corpus-small.train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yOZwLjo9bTeW","executionInfo":{"status":"ok","timestamp":1663921454672,"user_tz":-180,"elapsed":560,"user":{"displayName":"Айдар Валеев","userId":"02469296822751141251"}},"outputId":"c75bd58b-40dc-4990-e980-3e49d572b629"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["In/IN an/DT Oct./NNP 19/CD review/NN of/IN ``/`` The/DT Misanthrope/NN ''/'' at/IN Chicago/NNP 's/POS Goodman/NNP Theatre/NNP (/-LRB- ``/`` Revitalized/VBN Classics/NNS Take/VBP the/DT Stage/NN in/IN Windy/NNP City/NNP ,/, ''/'' Leisure/NN &/CC Arts/NNS )/-RRB- ,/, the/DT role/NN of/IN Celimene/NNP ,/, played/VBN by/IN Kim/NNP Cattrall/NNP ,/, was/VBD mistakenly/RB attributed/VBN to/TO Christina/NNP Haag/NNP ./.\n","Ms./NNP Haag/NNP plays/VBZ Elianti/NNP ./.\n","Rolls-Royce/NNP Motor/NNP Cars/NNPS Inc./NNP said/VBD it/PRP expects/VBZ its/PRP$ U.S./NNP sales/NNS to/TO remain/VB steady/JJ at/IN about/IN 1,200/CD cars/NNS in/IN 1990/CD ./.\n","The/DT luxury/NN auto/NN maker/NN last/JJ year/NN sold/VBD 1,214/CD cars/NNS in/IN the/DT U.S./NNP\n","Howard/NNP Mosher/NNP ,/, president/NN and/CC chief/JJ executive/NN officer/NN ,/, said/VBD he/PRP anticipates/VBZ growth/NN for/IN the/DT luxury/NN auto/NN maker/NN in/IN Britain/NNP and/CC Europe/NNP ,/, and/CC in/IN Far/JJ Eastern/JJ markets/NNS ./.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gEBMwWv7rxhC","outputId":"e98cddc0-e8f2-46ae-897a-aec5fa8ce82f","executionInfo":{"status":"ok","timestamp":1663921478482,"user_tz":-180,"elapsed":304,"user":{"displayName":"Айдар Валеев","userId":"02469296822751141251"}}},"source":["def prepare_sequence(seq, to_ix):  \n","    idxs = [to_ix[w] if w in to_ix else to_ix['the'] for w in seq ]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","def load_data(path):\n","    def process(line):\n","        ### YOUR CODE HERE ~7 lines\n","        tagged_words = line.split()\n","        words, tags = [], []\n","        for word_tag in tagged_words:\n","            w, t = word_tag.rsplit(\"/\", maxsplit=1)\n","            words.append(w)\n","            tags.append(t)\n","        return (words, tags)\n","    file = open(path, 'r')\n","    lines = file.read().split(\"\\n\")\n","    return [process(line) for line in lines if line]\n","\n","\n","training_data = [\n","    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n","    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n","]\n","training_data = load_data(\"corpus-small.train\") ### YOUR CODE HERE 1 line\n","\n","word_to_ix = {}\n","tag_to_ix = {}\n","for sent, tags in training_data:\n","    for word in sent:\n","        if word not in word_to_ix:\n","            word_to_ix[word] = len(word_to_ix)\n","\n","    # Write your code here to make fill the tag_to_ix\n","    ### YOUR CODE HERE ~3 lines\n","    for tag in tags:\n","        if tag not in tag_to_ix:\n","            tag_to_ix[tag] = len(tag_to_ix)\n","print(word_to_ix)\n","print(tag_to_ix)\n","# These will usually be more like 32 or 64 dimensional.\n","# We will keep them small, so we can see how the weights change as we train.\n","EMBEDDING_DIM = 6\n","HIDDEN_DIM = 6"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{'In': 0, 'an': 1, 'Oct.': 2, '19': 3, 'review': 4, 'of': 5, '``': 6, 'The': 7, 'Misanthrope': 8, \"''\": 9, 'at': 10, 'Chicago': 11, \"'s\": 12, 'Goodman': 13, 'Theatre': 14, '(': 15, 'Revitalized': 16, 'Classics': 17, 'Take': 18, 'the': 19, 'Stage': 20, 'in': 21, 'Windy': 22, 'City': 23, ',': 24, 'Leisure': 25, '&': 26, 'Arts': 27, ')': 28, 'role': 29, 'Celimene': 30, 'played': 31, 'by': 32, 'Kim': 33, 'Cattrall': 34, 'was': 35, 'mistakenly': 36, 'attributed': 37, 'to': 38, 'Christina': 39, 'Haag': 40, '.': 41, 'Ms.': 42, 'plays': 43, 'Elianti': 44, 'Rolls-Royce': 45, 'Motor': 46, 'Cars': 47, 'Inc.': 48, 'said': 49, 'it': 50, 'expects': 51, 'its': 52, 'U.S.': 53, 'sales': 54, 'remain': 55, 'steady': 56, 'about': 57, '1,200': 58, 'cars': 59, '1990': 60, 'luxury': 61, 'auto': 62, 'maker': 63, 'last': 64, 'year': 65, 'sold': 66, '1,214': 67, 'Howard': 68, 'Mosher': 69, 'president': 70, 'and': 71, 'chief': 72, 'executive': 73, 'officer': 74, 'he': 75, 'anticipates': 76, 'growth': 77, 'for': 78, 'Britain': 79, 'Europe': 80, 'Far': 81, 'Eastern': 82, 'markets': 83, 'BELL': 84, 'INDUSTRIES': 85, 'increased': 86, 'quarterly': 87, '10': 88, 'cents': 89, 'from': 90, 'seven': 91, 'a': 92, 'share': 93, 'new': 94, 'rate': 95, 'will': 96, 'be': 97, 'payable': 98, 'Feb.': 99, '15': 100, 'A': 101, 'record': 102, 'date': 103, 'has': 104, \"n't\": 105, 'been': 106, 'set': 107, 'Bell': 108, 'based': 109, 'Los': 110, 'Angeles': 111, 'makes': 112, 'distributes': 113, 'electronic': 114, 'computer': 115, 'building': 116, 'products': 117, 'Investors': 118, 'are': 119, 'appealing': 120, 'Securities': 121, 'Exchange': 122, 'Commission': 123, 'not': 124, 'limit': 125, 'their': 126, 'access': 127, 'information': 128, 'stock': 129, 'purchases': 130, 'corporate': 131, 'insiders': 132, 'SEC': 133, 'proposal': 134, 'ease': 135, 'reporting': 136, 'requirements': 137, 'some': 138, 'company': 139, 'executives': 140, 'would': 141, 'undermine': 142, 'usefulness': 143, 'on': 144, 'insider': 145, 'trades': 146, 'as': 147, 'stock-picking': 148, 'tool': 149, 'individual': 150, 'investors': 151, 'professional': 152, 'money': 153, 'managers': 154, 'contend': 155, 'They': 156, 'make': 157, 'argument': 158, 'letters': 159, 'agency': 160, 'rule': 161, 'changes': 162, 'proposed': 163, 'this': 164, 'past': 165, 'summer': 166, 'that': 167, 'among': 168, 'other': 169, 'things': 170, 'exempt': 171, 'many': 172, 'middle-management': 173, 'own': 174, 'companies': 175, \"'\": 176, 'shares': 177, 'also': 178, 'allow': 179, 'report': 180, 'exercises': 181, 'options': 182, 'later': 183, 'less': 184, 'often': 185, 'Many': 186, 'maintain': 187, 'investor': 188, 'confidence': 189, 'so': 190, 'shaken': 191, '1987': 192, 'market': 193, 'crash': 194, '--': 195, 'already': 196, 'stacked': 197, 'against': 198, 'little': 199, 'guy': 200, 'any': 201, 'decrease': 202, 'insider-trading': 203, 'patterns': 204, 'might': 205, 'prompt': 206, 'individuals': 207, 'get': 208, 'out': 209, 'stocks': 210, 'altogether': 211, 'historically': 212, 'paid': 213, 'obeisance': 214, 'ideal': 215, 'level': 216, 'playing': 217, 'field': 218, 'wrote': 219, 'Clyde': 220, 'S.': 221, 'McGregor': 222, 'Winnetka': 223, 'Ill.': 224, 'one': 225, '92': 226, 'received': 227, 'since': 228, 'were': 229, 'Aug.': 230, '17': 231, 'Apparently': 232, 'commission': 233, 'did': 234, 'really': 235, 'believe': 236, 'Currently': 237, 'rules': 238, 'force': 239, 'directors': 240, 'within': 241, 'month': 242, 'after': 243, 'transaction': 244, 'But': 245, '25': 246, '%': 247, 'according': 248, 'figures': 249, 'file': 250, 'reports': 251, 'late': 252, 'effort': 253, 'streamline': 254, 'federal': 255, 'bureaucracy': 256, 'boost': 257, 'compliance': 258, 'who': 259, 'calling': 260, 'shots': 261, 'Brian': 262, 'Lane': 263, 'special': 264, 'counsel': 265, 'office': 266, 'disclosure': 267, 'policy': 268, 'which': 269, 'officials': 270, 'had': 271, 'until': 272, 'today': 273, 'comment': 274, 'proposals': 275, 'issue': 276, 'produced': 277, 'more': 278, 'mail': 279, 'than': 280, 'almost': 281, 'memory': 282, 'Mr.': 283, 'probably': 284, 'vote': 285, 'early': 286, 'next': 287, 'Not': 288, 'all': 289, 'those': 290, 'oppose': 291, 'Committee': 292, 'Federal': 293, 'Regulation': 294, 'American': 295, 'Bar': 296, 'Association': 297, 'argues': 298, 'example': 299, 'lengthy': 300, 'letter': 301, 'substantially': 302, 'improve': 303, '{': 304, 'law': 305, '}': 306, 'conforming': 307, 'closely': 308, 'contemporary': 309, 'business': 310, 'realities': 311, 'What': 312, 'object': 313, 'most': 314, 'is': 315, 'effect': 316, 'they': 317, 'say': 318, 'have': 319, 'ability': 320, 'spot': 321, 'telltale': 322, 'clusters': 323, 'trading': 324, 'activity': 325, 'buying': 326, 'or': 327, 'selling': 328, 'director': 329, 'short': 330, 'period': 331, 'time': 332, 'According': 333, 'estimates': 334, 'cut': 335, 'filings': 336, 'third': 337, 'vehemently': 338, 'disputed': 339, 'eliminate': 340, 'policy-making': 341, 'divisions': 342, 'such': 343, 'marketing': 344, 'finance': 345, 'research': 346, 'development': 347, 'tougher': 348, 'still': 349, 'required': 350, 'Companies': 351, 'compelled': 352, 'publish': 353, 'annual': 354, 'proxy': 355, 'statements': 356, 'names': 357, 'fail': 358, 'Considered': 359, 'whole': 360, 'under': 361, 'least': 362, 'effective': 363, 'if': 364, 'following': 365, 'transactions': 366, 'Robert': 367, 'Gabele': 368, 'Invest/Net': 369, 'North': 370, 'Miami': 371, 'Fla.': 372, 'packages': 373, 'sells': 374, 'data': 375, 'worded': 376, 'vaguely': 377, 'key': 378, 'may': 379, 'asking': 380, 'require': 381, 'immediately': 382, 'while': 383, 'regulates': 384, 'files': 385, 'tells': 386, 'them': 387, 'when': 388, 'do': 389, 'want': 390, 'change': 391, 'timing': 392, 'should': 393, 'write': 394, 'representatives': 395, 'Congress': 396, 'added': 397, 'likely': 398, 'amenable': 399, 'legislation': 400, 'timely': 401, 'basis': 402, 'nation': 403, 'largest': 404, 'pension': 405, 'fund': 406, 'oversees': 407, '$': 408, '80': 409, 'billion': 410, 'college': 411, 'employees': 412, 'plans': 413, 'offer': 414, 'two': 415, 'investment': 416, '1.2': 417, 'million': 418, 'participants': 419, 'Teachers': 420, 'Insurance': 421, 'Annuity': 422, 'Association-College': 423, 'Retirement': 424, 'Equities': 425, 'Fund': 426, 'introduce': 427, 'bond': 428, 'invest': 429, 'socially': 430, 'responsible': 431, 'Both': 432, 'funds': 433, 'expected': 434, 'begin': 435, 'operation': 436, 'around': 437, 'March': 438, '1': 439, 'subject': 440, 'approval': 441, 'For': 442, 'sign': 443, 'up': 444, 'must': 445, 'approve': 446, 'plan': 447, 'Some': 448, '4,300': 449, 'institutions': 450, 'part': 451, 'carry': 452, 'agreement': 453, 'pressure': 454, 'relax': 455, 'strict': 456, 'participation': 457, 'provide': 458, 'reached': 459, 'with': 460, 'December': 461, 'social': 462, 'choice': 463, 'shun': 464, 'securities': 465, 'linked': 466, 'South': 467, 'Africa': 468, 'nuclear': 469, 'power': 470, 'cases': 471, 'Northern': 472, 'Ireland': 473, 'Also': 474, 'excluded': 475, 'investments': 476, 'significant': 477, 'stemming': 478, 'weapons': 479, 'manufacture': 480, 'alcoholic': 481, 'beverages': 482, 'tobacco': 483, 'Sixty': 484, 'percent': 485, 'invested': 486, 'rest': 487, 'going': 488, 'into': 489, 'bonds': 490, 'short-term': 491, 'high-grade': 492, 'medium-grade': 493, 'mortgages': 494, 'asset-backed': 495, 'including': 496, 'much': 497, 'foreign': 498, 'buy': 499, 'sell': 500, 'futures': 501, 'contracts': 502, 'New': 503, 'York': 504, 'State': 505, 'Department': 506, 'Under': 507, 'features': 508, 'able': 509, 'transfer': 510, 'jobs': 511, 'terminated': 512, 'receive': 513, 'cash': 514, 'choices': 515, 'offered': 516, 'currently': 517, 'limited': 518, 'annuity': 519, 'money-market': 520, 'Brunswick': 521, 'Scientific': 522, 'Co.': 523, 'biotechnology': 524, 'instrumentation': 525, 'equipment': 526, 'adopted': 527, 'anti-takeover': 528, 'giving': 529, 'shareholders': 530, 'right': 531, 'purchase': 532, 'half': 533, 'price': 534, 'certain': 535, 'conditions': 536, 'protect': 537, 'abusive': 538, 'takeover': 539, 'tactics': 540, 'W.': 541, 'Ed': 542, 'Tyler': 543, '37': 544, 'years': 545, 'old': 546, 'senior': 547, 'vice': 548, 'printing': 549, 'concern': 550, 'elected': 551, 'technology': 552, 'group': 553, 'position': 554, 'Solo': 555, 'woodwind': 556, 'players': 557, 'creative': 558, 'work': 559, 'lot': 560, 'because': 561, 'repertoire': 562, 'audience': 563, 'appeal': 564, 'oboist': 565, 'Heinz': 566, 'Holliger': 567, 'taken': 568, 'hard': 569, 'line': 570, 'problem': 571, ':': 572, 'He': 573, 'commissions': 574, 'splendidly': 575, 'interprets': 576, 'fearsome': 577, 'scores': 578, 'does': 579, 'conducting': 580, 'play': 581, 'same': 582, 'Mozart': 583, 'Strauss': 584, 'concertos': 585, 'over': 586, 'again': 587, 'Richard': 588, 'Stoltzman': 589, 'gentler': 590, 'audience-friendly': 591, 'approach': 592, 'Years': 593, 'ago': 594, 'collaborated': 595, 'music': 596, 'gurus': 597, 'Peter': 598, 'Serkin': 599, 'Fred': 600, 'Sherry': 601, 'very': 602, 'countercultural': 603, 'chamber': 604, 'Tashi': 605, 'won': 606, 'audiences': 607, 'dreaded': 608, 'like': 609, 'Messiaen': 610, 'Quartet': 611, 'End': 612, 'Time': 613, 'Today': 614, 'pixie-like': 615, 'clarinetist': 616, 'mostly': 617, 'dropped': 618, 'missionary': 619, 'though': 620, 'touch': 621, 'survives': 622, 'now': 623, 'goes': 624, 'road': 625, 'piano': 626, 'bass': 627, 'slide': 628, 'show': 629, 'ranges': 630, 'light': 631, 'classical': 632, 'jazz': 633, 'pop': 634, 'few': 635, 'notable': 636, 'exceptions': 637, 'Just': 638, 'thing': 639, 'Vivaldi-at-brunch': 640, 'yuppie': 641, 'embraced': 642, 'Age': 643, 'easy': 644, 'listening': 645, 'you': 646, 'ca': 647, 'dismiss': 648, 'his': 649, 'motives': 650, 'merely': 651, 'commercial': 652, 'lightweight': 653, 'believes': 654, 'what': 655, 'superbly': 656, 'His': 657, 'recent': 658, 'appearance': 659, 'Metropolitan': 660, 'Museum': 661, 'dubbed': 662, 'Musical': 663, 'Odyssey': 664, 'case': 665, 'point': 666, 'It': 667, 'felt': 668, 'party': 669, 'highly': 670, 'polished': 671, 'jam': 672, 'session': 673, 'friends': 674, 'concert': 675, 'Clad': 676, 'trademark': 677, 'black': 678, 'velvet': 679, 'suit': 680, 'soft-spoken': 681, 'announced': 682, 'album': 683, 'Inner': 684, 'Voices': 685, 'just': 686, 'released': 687, 'family': 688, 'front': 689, 'row': 690, 'mother': 691, 'birthday': 692, 'her': 693, 'favorite': 694, 'tune': 695, 'launched': 696, 'Saint-Saens': 697, 'Swan': 698, 'Carnival': 699, 'Animals': 700, 'encore': 701, 'piece': 702, 'cellists': 703, 'lovely': 704, 'glossy': 705, 'tone': 706, 'no': 707, 'bite': 708, 'Then': 709, 'could': 710, 'fast': 711, 'well': 712, 'second': 713, 'movement': 714, 'Sonata': 715, 'Clarinet': 716, 'whimsical': 717, 'puckish': 718, 'tidbit': 719, 'reflected': 720, 'flip': 721, 'side': 722, 'personality': 723, 'And': 724, 'went': 725, 'through': 726, 'first': 727, 'ingeniously': 728, 'chosen': 729, 'potpourri': 730, 'pieces': 731, 'none': 732, 'longer': 733, 'five': 734, 'minutes': 735, 'disturb': 736, 'challenge': 737, 'listener': 738, 'introduced': 739, 'colleagues': 740, 'Bill': 741, 'Douglas': 742, 'pianist/bassoonist/composer': 743, 'buddy': 744, 'Yale': 745, 'bassist': 746, 'Eddie': 747, 'Gomez': 748, 'An': 749, 'improvisational': 750, 'section': 751, 'built': 752, 'beginning': 753, 'Golden': 754, 'Rain': 755, 'lilting': 756, 'laid-back': 757, 'lead': 758, 'uptempo': 759, 'Sky': 760, 'gave': 761, 'opportunity': 762, 'wail': 763, 'high': 764, 'register': 765, 'off': 766, 'fleet': 767, 'fingers': 768, 'Bach': 769, 'Air': 770, 'followed': 771, 'tied': 772, 'composer': 773, 'proclaiming': 774, 'him': 775, 'great': 776, 'improviser': 777, '18th': 778, 'century': 779, 'then': 780, 'image': 781, 'joining': 782, 'two-part': 783, 'inventions': 784, 'cleverly': 785, 'arranged': 786, 'clarinet': 787, 'bassoon': 788, 'Keeping': 789, 'mood': 790, 'chanted': 791, 'chortled': 792, 'way': 793, 'murderous': 794, 'polyrhythms': 795, 'devised': 796, 'alternative': 797, 'Hindemith': 798, 'dry': 799, 'theory-teaching': 800, 'techniques': 801, 'soared': 802, 'improvised': 803, 'tight': 804, 'Bebop': 805, 'Etudes': 806, 'end': 807, 'however': 808, 'brought': 809, 'standing-room-only': 810, 'crowd': 811, 'seemed': 812, 'waiting': 813, 'singer': 814, 'Judy': 815, 'Collins': 816, 'appears': 817, 'Glamorous': 818, 'pure-voiced': 819, 'ever': 820, 'sang': 821, 'Joni': 822, 'Mitchell': 823, 'Free': 824, 'encounter': 825, 'street-corner': 826, 'contributed': 827, 'obligatto': 828, 'lush': 829, 'setting': 830, 'Gaelic': 831, 'blessing': 832, 'Deep': 833, 'Peace': 834, 'featured': 835, 'but': 836, 'predictable': 837, 'images': 838, 'clouds': 839, 'beaches': 840, 'deserts': 841, 'sunsets': 842, 'etc': 843, 'too': 844, 'mellow': 845, 'believed': 846, 'gotten': 847, 'away': 848, 'add': 849, 'signature': 850, 'Amazing': 851, 'Grace': 852, 'ask': 853, 'That': 854, 'permissible': 855, 'warm': 856, 'fuzzy': 857, 'feelings': 858, 'Was': 859, 'why': 860, 'departed': 861, 'before': 862, 'during': 863, '?': 864, 'Or': 865, 'gone': 866, 'Either': 867, 'pity': 868, 'Stolzman': 869, 'substantial': 870, 'evening': 871, 'intermission': 872, 'Steve': 873, 'Reich': 874, 'Counterpoint': 875, 'series': 876, 'works': 877, 'juxtapose': 878, 'live': 879, 'performer': 880, 'recorded': 881, 'tracks': 882, 'Different': 883, 'Trains': 884, 'string': 885, 'quartet': 886, 'uses': 887, 'technique': 888, 'magisterially': 889, 'worried': 890, 'take': 891, 'warned': 892, 'us': 893, 'advance': 894, 'lasts': 895, '11': 896, '1/2': 897, 'unfortunately': 898, 'illustrated': 899, 'intricate': 900, 'jazzy': 901, 'tapestry': 902, 'Pearson': 903, 'geometric': 904, 'repeating': 905, 'objects': 906, 'kitschy': 907, 'mirroring': 908, 'musical': 909, 'structure': 910, 'thoroughly': 911, 'distracting': 912, 'elegant': 913, 'execution': 914, 'straight': 915, 'sounds': 916, 'written': 917, 'Charlie': 918, 'Parker': 919, 'Ornette': 920, 'Coleman': 921, 'pictures': 922, 'enjoyable': 923, 'hear': 924, 'accomplished': 925, 'without': 926, 'having': 927, 'sit': 928, 'smoke-filled': 929, 'club': 930, 'ultimately': 931, 'forgettable': 932, 'Is': 933, 'future': 934, 'Managers': 935, 'presenters': 936, 'insist': 937, 'concerts': 938, 'can': 939, 'enjoy': 940, 'only': 941, 'purged': 942, 'threatening': 943, 'elements': 944, 'served': 945, 'bite-sized': 946, 'morsels': 947, 'accompanied': 948, 'visuals': 949, 'Slides': 950, 'illustrate': 951, 'Shostakovich': 952, 'quartets': 953, 'unpleasant': 954, 'certainly': 955, 'thanks': 956, 'performance': 957, 'compositional': 958, 'talents': 959, 'obvious': 960, 'sincerity': 961, 'chooses': 962, 'selections': 963, 'neither': 964, 'deep': 965, 'nor': 966, 'lasting': 967, 'entertainment': 968, 'substitute': 969, 'Brahms': 970, 'Waleson': 971, 'free-lance': 972, 'writer': 973, 'One': 974, 'Ronald': 975, 'Reagan': 976, 'attributes': 977, 'President': 978, 'rarely': 979, 'claptrap': 980, 'passes': 981, 'consensus': 982, 'various': 983, 'international': 984, 'fact': 985, 'liberated': 986, 'world': 987, 'corrupt': 988, 'organizations': 989, 'UNESCO': 990, 'This': 991, 'U.N.': 992, 'managed': 993, 'traduce': 994, 'charter': 995, 'promoting': 996, 'education': 997, 'science': 998, 'culture': 999, 'Ever': 1000, 'remaining': 1001, 'members': 1002, 'desperate': 1003, 'United': 1004, 'States': 1005, 'rejoin': 1006, 'dreadful': 1007, 'Now': 1008, 'apologists': 1009, 'lobbying': 1010, 'Bush': 1011, 'renege': 1012, 'decision': 1013, 'depart': 1014, 'we': 1015, 'think': 1016, 'reasons': 1017, 'stay': 1018, 'foreseeable': 1019, 'beyond': 1020, 'along': 1021, 'Singapore': 1022, 'left': 1023, 'anti-Western': 1024, 'ideology': 1025, 'financial': 1026, 'corruption': 1027, 'top': 1028, 'leadership': 1029, 'got': 1030, 'hand': 1031, 'personal': 1032, 'antics': 1033, 'Director': 1034, 'Amadou-Mahtar': 1035, \"M'Bow\": 1036, 'drew': 1037, 'attention': 1038, 'several': 1039, 'aides': 1040, 'uncovered': 1041, 'KGB': 1042, 'plants': 1043, 'ejected': 1044, 'France': 1045, 'mysterious': 1046, 'fire': 1047, 'sent': 1048, 'accountants': 1049, 'trace': 1050, 'extreme': 1051, 'even': 1052, 'replacement': 1053, 'personally': 1054, 'genial': 1055, 'Spanish': 1056, 'biochemist': 1057, 'Federico': 1058, 'Mayor': 1059, 'success': 1060, 'achieving': 1061, 'reforms': 1062, 'Several': 1063, 'ridiculous': 1064, 'projects': 1065, 'continue': 1066, 'International': 1067, 'Economic': 1068, 'Order': 1069, 'means': 1070, 'redistributionism': 1071, 'West': 1072, 'pay': 1073, 'everyone': 1074, 'else': 1075, 'statism': 1076}\n","{'IN': 0, 'DT': 1, 'NNP': 2, 'CD': 3, 'NN': 4, '``': 5, \"''\": 6, 'POS': 7, '-LRB-': 8, 'VBN': 9, 'NNS': 10, 'VBP': 11, ',': 12, 'CC': 13, '-RRB-': 14, 'VBD': 15, 'RB': 16, 'TO': 17, '.': 18, 'VBZ': 19, 'NNPS': 20, 'PRP': 21, 'PRP$': 22, 'VB': 23, 'JJ': 24, 'MD': 25, 'VBG': 26, 'RBR': 27, ':': 28, 'WP': 29, 'WDT': 30, 'JJR': 31, 'PDT': 32, 'RBS': 33, 'WRB': 34, 'JJS': 35, '$': 36, 'RP': 37, 'FW': 38}\n"]}]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"Y8eNxgmdrxhE"},"source":["Task:\n","Add dropout inside the LSTM module."]},{"cell_type":"code","metadata":{"id":"TiWg7JYerxhG","executionInfo":{"status":"ok","timestamp":1663921496032,"user_tz":-180,"elapsed":259,"user":{"displayName":"Айдар Валеев","userId":"02469296822751141251"}}},"source":["class LSTMTagger(nn.Module):\n","\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n","        super(LSTMTagger, self).__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n","\n","        # The LSTM takes word embeddings as inputs, and outputs hidden states\n","        # with dimensionality hidden_dim.\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, dropout=0.2) ### CHANGE THIS LINE\n","\n","        # The linear layer that maps from hidden state space to tag space\n","        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n","\n","    def forward(self, sentence):\n","        embeds = self.word_embeddings(sentence)\n","        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n","        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n","        tag_scores = F.log_softmax(tag_space, dim=1)\n","        return tag_scores"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qkfP5YOkrxhI","outputId":"3c07ba5e-5dd2-4f75-9f77-1a67a92a6e11","executionInfo":{"status":"ok","timestamp":1663921634567,"user_tz":-180,"elapsed":138194,"user":{"displayName":"Айдар Валеев","userId":"02469296822751141251"}}},"source":["model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n","loss_function = nn.NLLLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","\n","# See what the scores are before training\n","# Note that element i,j of the output is the score for tag j for word i.\n","# Here we don't need to train, so the code is wrapped in torch.no_grad()\n","with torch.no_grad():\n","    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n","    tag_scores = model(inputs)\n","    print(tag_scores)\n","\n","### Reduce number of epochs, if training data is big\n","for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n","    for sentence, tags in training_data:\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is, turn them into\n","        # Tensors of word indices.\n","        sentence_in = prepare_sequence(sentence, word_to_ix)\n","        targets = prepare_sequence(tags, tag_to_ix)\n","\n","        # Step 3. Run our forward pass.\n","        tag_scores = model(sentence_in)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss = loss_function(tag_scores, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","# See what the scores are after training\n","import numpy as np\n","with torch.no_grad():\n","    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n","    tag_scores = model(inputs)\n","    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n","    # for word i. The predicted tag is the maximum scoring tag.\n","    # Here, we can see the predicted sequence below is 0 1 2 0 1\n","    # since 0 is index of the maximum value of row 1,\n","    # 1 is the index of maximum value of row 2, etc.\n","    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n","    print(tag_scores)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[-3.7736, -3.4513, -3.7759,  ..., -3.6650, -3.9193, -3.7274],\n","        [-3.7482, -3.4301, -3.7756,  ..., -3.6177, -3.9206, -3.6928],\n","        [-3.8730, -3.4990, -3.7137,  ..., -3.6426, -4.0071, -3.8072],\n","        ...,\n","        [-3.8692, -3.4937, -3.7748,  ..., -3.5918, -3.9295, -3.6922],\n","        [-3.8353, -3.5286, -3.9019,  ..., -3.5698, -3.9514, -3.8075],\n","        [-3.6774, -3.3781, -3.8495,  ..., -3.6257, -3.9517, -3.7885]])\n","tensor([[-9.4084e-01, -1.5780e+01, -1.0764e+01,  ..., -9.9006e+00,\n","         -7.1127e+00, -1.0230e+01],\n","        [-1.3622e+01, -1.7584e-02, -1.1248e+01,  ..., -1.2287e+01,\n","         -8.9040e+00, -1.0716e+01],\n","        [-1.2903e+01, -1.1716e+01, -9.1159e-02,  ..., -1.0576e+01,\n","         -1.0262e+01, -1.0857e+01],\n","        ...,\n","        [-1.0035e+01, -1.1695e+01, -1.6151e+00,  ..., -9.3889e+00,\n","         -7.5078e+00, -9.7556e+00],\n","        [-1.7440e+01, -5.9350e+00, -1.5731e-01,  ..., -1.3054e+01,\n","         -1.0158e+01, -1.2978e+01],\n","        [-4.9526e+00, -7.2188e+00, -2.3977e+01,  ..., -1.5334e+01,\n","         -1.0385e+01, -1.3836e+01]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"moNkSu6qrxhM"},"source":["**Task:**\n","\n","Read the test data `corpus-small.test` and process it then get the predicitions.Write down the output tagged predicitons in file `corpus-small.out` in the same form as `corpus-small.answer`.\n","\n","Note: in-case of unseen word in the testing dataset, replace it with a random seen one! (There's a better solution).\n","\n","At the end, run the last cell, to get the accuracy of your model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-HNIDCWnrxhP","outputId":"d59fcc81-40d9-4a1e-f045-c56df6056d8f","executionInfo":{"status":"ok","timestamp":1663921912390,"user_tz":-180,"elapsed":276,"user":{"displayName":"Айдар Валеев","userId":"02469296822751141251"}}},"source":["def load_test_data(path):\n","    with open(path, 'r') as f:\n","      return [line.split() for line in f.readlines() if line]  ### FINISH THIS LINE\n","\n","test_data = load_test_data('corpus-small.test')\n","\n","reverse_tag = { v : k for k , v in tag_to_ix.items()}\n","\n","answers = []\n","model.zero_grad()\n","model.eval()\n","with torch.no_grad():\n","    for sentence in test_data:\n","        # Write your code here to make the predictions.\n","        sentence_in = prepare_sequence(sentence, word_to_ix)\n","        tag_scores =  model(sentence_in)\n","        labels = torch.argmax(tag_scores,dim=1).detach().cpu().numpy()\n","        tagss = [reverse_tag[ids] for ids in labels]\n","        results = \" \".join([w + \"/\" + t for w, t in zip(sentence, tagss)])\n","        answers.append(results)\n","\n","print(answers)\n","file = open('corpus-small.out','w')\n","file.write(\"\\n\".join(answers))\n","file.close()"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["['For/IN six/DT years/NNS ,/, T./DT Marshall/DT Hahn/DT Jr./DT has/NN made/DT corporate/JJ acquisitions/DT in/JJ the/DT George/DT Bush/JJ mode/DT :/VBP kind/DT and/CC gentle/DT ./.', \"The/DT question/DT now/VBD :/'' Can/DT he/PRP act/DT more/JJR like/IN hard-charging/DT Teddy/DT Roosevelt/DT ?/.\", \"Mr./NNP Hahn/DT ,/, the/DT 62-year-old/DT chairman/DT and/CC chief/JJ executive/NN officer/NN of/IN Georgia-Pacific/DT Corp./DT is/VBZ leading/DT the/DT forest-product/DT concern/NN 's/POS unsolicited/DT $/NN 3.19/DT billion/JJ bid/DT for/IN Great/DT Northern/NNP Nekoosa/DT Corp/DT ./.\", \"Nekoosa/DT has/VBZ given/DT the/DT offer/IN a/DT public/DT cold/DT shoulder/DT ,/, a/DT reaction/DT Mr./NNP Hahn/DT has/VBZ n't/RB faced/DT in/JJ his/PRP$ 18/DT earlier/DT acquisitions/DT ,/, all/VBN of/IN which/IN were/VBD negotiated/DT behind/DT the/DT scenes/DT ./.\", 'So/DT far/DT ,/, Mr./NNP Hahn/DT is/VBZ trying/DT to/NNP entice/DT Nekoosa/DT into/IN negotiating/DT a/DT friendly/DT surrender/DT while/JJ talking/DT tough/DT ./.', \"``/`` We/DT are/VBP prepared/DT to/NNP pursue/DT aggressively/DT completion/DT of/IN this/DT transaction/NN ,/, ''/'' he/PRP says/DT ./.\", 'But/CC a/DT takeover/JJ battle/DT opens/DT up/NN the/DT possibility/DT of/IN a/DT bidding/DT war/DT ,/, with/IN all/VBD that/IN implies/DT ./.', 'If/DT a/DT competitor/DT enters/DT the/DT game/DT ,/, for/IN example/NN ,/, Mr./NNP Hahn/DT could/MD face/DT the/DT dilemma/DT of/IN paying/DT a/DT premium/DT for/IN Nekoosa/DT or/CC seeing/DT the/DT company/NN fall/DT into/IN the/DT arms/DT of/IN a/DT rival/DT ./.', 'Given/DT that/WDT choice/VBD ,/, associates/DT of/IN Mr./NNP Hahn/DT and/CC industry/DT observers/DT say/JJ the/DT former/DT university/DT president/NN --/: who/WP has/VBZ developed/DT a/DT reputation/DT for/IN not/RB overpaying/DT for/IN anything/DT --/MD would/MD fold/DT ./.', \"Says/DT long-time/DT associate/DT Jerry/DT Griffin/DT ,/, vice/NN president/NN ,/, corporate/JJ development/NN ,/, at/IN WTD/DT Industries/DT Inc./NNP :/'' ``/`` He/PRP is/VBZ n't/RB of/IN the/DT old/JJ school/DT of/IN winning/DT at/IN any/DT cost/DT ./. ''/''\", 'He/PRP also/VBD is/VBZ a/DT consensus/JJ manager/DT ,/, insiders/NNS say/JJ ./.', \"The/DT decision/NN to/TO make/VB the/DT bid/DT for/IN Nekoosa/DT ,/, for/IN example/NN ,/, was/VBD made/DT only/PRP$ after/CC all/NNP six/DT members/NN of/IN Georgia-Pacific/DT 's/NN management/DT committee/DT signed/DT onto/DT the/DT deal/DT --/MD even/NNS though/IN Mr./NNP Hahn/DT knew/DT he/PRP wanted/DT to/NNP go/DT after/NNS the/DT company/NN early/RB on/IN ,/, says/DT Mr./NNP Correll/DT ./.\", 'Associates/DT say/JJ Mr./NNP Hahn/DT picked/DT up/NN that/WDT careful/DT approach/NN to/TO management/DT as/JJ president/NN of/IN Virginia/DT Polytechnic/DT Institute/DT ./.', 'Assuming/DT that/WDT post/DT at/IN the/DT age/DT of/IN 35/DT ,/, he/PRP managed/NNS by/IN consensus/NN ,/, as/IN is/VBZ the/DT rule/NN in/DT universities/DT ,/, says/DT Warren/DT H./DT Strother/DT ,/, a/DT university/DT official/DT who/WP is/VBZ researching/DT a/DT book/DT on/IN Mr./NNP Hahn/DT ./.', 'But/CC he/PRP also/RB showed/DT a/DT willingness/DT to/NNP take/VB a/DT strong/DT stand/DT ./.', 'In/IN 1970/DT ,/, Mr./NNP Hahn/DT called/DT in/IN state/DT police/DT to/NNP arrest/DT student/DT protesters/DT who/WP were/VBD occupying/DT a/DT university/DT building/NN ./.', \"That/WDT impressed/DT Robert/NNP B./DT Pamplin/DT ,/, Georgia-Pacific/DT 's/NN chief/JJ executive/NN at/IN the/DT time/NN ,/, whom/DT Mr./NNP Hahn/DT had/VBD met/DT while/JJ fundraising/DT for/IN the/DT institute/DT ./.\", \"In/IN 1975/DT ,/, Mr./NNP Pamplin/DT enticed/DT Mr./NNP Hahn/DT into/IN joining/JJ the/DT company/NN as/IN executive/NN vice/NN president/NN in/IN charge/DT of/IN chemicals/DT ;/DT the/DT move/DT befuddled/DT many/JJ in/IN Georgia-Pacific/DT who/WP did/VBD n't/RB believe/VB a/DT university/DT administrator/DT could/NN make/NN the/DT transition/DT to/NNP the/DT corporate/JJ world/NNS ./.\", 'But/CC Mr./NNP Hahn/DT rose/DT swiftly/DT through/NNS the/DT ranks/DT ,/, demonstrating/DT a/DT raw/DT intelligence/DT that/WDT he/PRP says/DT he/PRP knew/DT he/PRP possessed/DT early/RB on/IN ./.', 'The/DT son/DT of/IN a/DT physicist/DT ,/, Mr./NNP Hahn/DT skipped/DT first/JJ grade/DT because/IN his/PRP$ reading/DT ability/NN was/VBD so/RB far/DT above/DT his/PRP$ classmates/DT ./.']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PmU2qv-RrxhR","outputId":"cb00a5cd-921b-4f94-d8a3-e9d241c864f5","executionInfo":{"status":"ok","timestamp":1663921913591,"user_tz":-180,"elapsed":5,"user":{"displayName":"Айдар Валеев","userId":"02469296822751141251"}}},"source":["%run tagger_eval.py corpus-small.out corpus-small.answer"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy= 0.4881422924901186\n"]}]},{"cell_type":"code","metadata":{"id":"JoCvLd56rxhS"},"source":[],"execution_count":null,"outputs":[]}]}